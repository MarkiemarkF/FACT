{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from test_fair_clustering import main\n",
    "import argparse\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_FOLDER = \"outputs\"\n",
    "CSV_NAME = \"results.csv\"\n",
    "CSV_NAME_LIPSCHITZ = \"results_Lipschitz.csv\"\n",
    "\n",
    "lambdas_by_setting = {\n",
    "    \"Synthetic\": {\n",
    "        \"kmedian\": 600, # Theirs: 10 got consistently bad results\n",
    "        \"kmeans\": 100, # Theirs: 10 got optimal results only on some seeds\n",
    "        # \"ncut\": 10,\n",
    "    }, \n",
    "    \"Synthetic-unequal\": {\n",
    "        \"kmedian\": 10,\n",
    "        \"kmeans\": 10, # 20,22,24 are outliers\n",
    "        # \"ncut\": 10,\n",
    "    }, \n",
    "    \"Adult\": {\n",
    "        \"kmedian\": 9000,\n",
    "        \"kmeans\": 9000,\n",
    "        # \"ncut\": 1.5,\n",
    "        # \"ncut\": 10, # Theirs with Lipschitz 1.0\n",
    "    }, \n",
    "    \"Bank\": {\n",
    "        \"kmedian\": 9000,\n",
    "        \"kmeans\": 6000,\n",
    "        # \"ncut\": 1,\n",
    "        # \"ncut\": 40, # Theirs with Lipschitz 1.0\n",
    "    }, \n",
    "    \"CensusII\": {\n",
    "        # \"kmedian\": 500000,\n",
    "        # \"kmeans\": 500000,\n",
    "        # \"ncut\": 2.5,\n",
    "        \"ncut\": 100, # Theirs with Lipschitz 1.0\n",
    "    }\n",
    "}\n",
    "\n",
    "n_runs = { # Number of runs to do per dataset\n",
    "    \"Synthetic\": 30, \n",
    "    \"Synthetic-unequal\": 30, \n",
    "    \"Adult\": 10,\n",
    "    \"Bank\": 10,\n",
    "    \"CensusII\": 5,\n",
    "}\n",
    "n_runs_Lipschitz = 10\n",
    "Lipschitz_constants = [0.00001, 0.0001, 0.001, 0.01, 0.1, 1.0, 2.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args(seed=1, dataset=\"Synthetic-unequal\", cluster_option=\"ncut\", lmbda=10, lmbda_tune=False, Lipschitz=None, plot_bound_update=False):\n",
    "    \"\"\"\n",
    "    Return Namespace instance with passed and default arguments for main().\n",
    "\n",
    "    plot_bound_update:\n",
    "        if True, lets main() return a list of energies of a single bound update. Meant for checking convergence.\n",
    "    \"\"\"\n",
    "    args = argparse.Namespace()\n",
    "    \n",
    "    args.plot_option_clusters_vs_lambda = False     # Only available for the synthetic datasets\n",
    "    args.plot_option_fairness_vs_clusterE = True    # Only works when lmbda_tune == True\n",
    "    args.plot_option_balance_vs_clusterE = False    # Only works when lmbda_tune == True\n",
    "    args.plot_option_convergence = False\n",
    "    args.plot_bound_update = plot_bound_update      # Return a list of the energies of a single bound update\n",
    "    \n",
    "    args.reprod = False\n",
    "    args.kernel_type = None\n",
    "    args.kernel_args = [1, 2]\n",
    "\n",
    "    args.seed = seed\n",
    "    args.dataset = dataset\n",
    "    args.cluster_option = cluster_option\n",
    "    args.lmbda = lmbda\n",
    "    args.lmbda_tune = lmbda_tune\n",
    "    \n",
    "    if Lipschitz:\n",
    "        args.L = Lipschitz\n",
    "    elif cluster_option == \"ncut\" and \"Synthetic\" not in dataset:\n",
    "        args.L = 1.0\n",
    "    else:\n",
    "        args.L = 2.0\n",
    "\n",
    "    working_dir = os.getcwd()\n",
    "    args.data_dir = os.path.join(working_dir, \"data\")\n",
    "    args.output_path = os.path.join(working_dir, OUTPUT_FOLDER)\n",
    "    return args\n",
    "\n",
    "def make_csv(dir_path, csv_path, fieldnames):\n",
    "    \"\"\"\n",
    "    Make path and csv file with header if it doesn't exist yet. Otherwise do nothing.\n",
    "\n",
    "    dir_path:\n",
    "        full path to directory\n",
    "    csv_path:\n",
    "        full path to directory including the csv file\n",
    "    fieldnames:\n",
    "        fieldnames for the csv\n",
    "    \"\"\"\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "    if os.path.isfile(csv_path):\n",
    "        with open(csv_path, \"r\") as f:\n",
    "            reader = csv.reader(f)\n",
    "            if sum(1 for row in reader) > 0:\n",
    "                return\n",
    "\n",
    "    with open(csv_path, \"w\", newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "def run_main(args, csv_name=CSV_NAME):\n",
    "    \"\"\"\n",
    "    Run main and append results with settings to csv file.\n",
    "    File is automatically made if it doesn't exist yet.\n",
    "\n",
    "    csv_name:\n",
    "        just the filename of the csv, eg \"results.csv\"\n",
    "    \"\"\"\n",
    "    results = main(args, logging=False, seedable=True)\n",
    "\n",
    "    save_dict = {\n",
    "        \"dataset\": args.dataset,\n",
    "        \"N\": results['N'],                                      # Dataset size\n",
    "        \"J\": results['J'],                                      # Number of demographic groups (defined in dataset_load.py)\n",
    "        \"lmbda\": args.lmbda,\n",
    "        \"Objective\": results[\"clustering energy (Objective)\"],  # Discrete clustering energy\n",
    "        \"fairness error\": results[\"fairness error\"],\n",
    "        \"balance\": results[\"balance\"],\n",
    "        \"cluster_option\": args.cluster_option,\n",
    "        \"time\": results[\"time\"],                                # Time taken to finish this run\n",
    "        \"seed\": args.seed,\n",
    "        \"lmbda_tune\": args.lmbda_tune,                          \n",
    "        \"K\": results['K'],                                      # Number of clusters (defined in dataset_load.py)\n",
    "        \"L\": args.L,                                            # Lipschitz constant\n",
    "    }\n",
    "\n",
    "    csv_path = os.path.join(args.output_path, csv_name)\n",
    "    fieldnames = save_dict.keys()\n",
    "    make_csv(args.output_path, csv_path, fieldnames)\n",
    "    with open(csv_path, \"a\", newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames)\n",
    "        writer.writerow(save_dict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_entry(args, entry, keys):\n",
    "    \"\"\"\n",
    "    Compare args to an entry from the csv file and return True if they have the given keys in common:\n",
    "\n",
    "    args:\n",
    "        arguments that will be passed to main()\n",
    "    entry:\n",
    "        entry in the csv file made by run_main()\n",
    "    \"\"\"\n",
    "    for key in keys:\n",
    "        if str(getattr(args, key)) != entry[key]:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def find_same_options(csv_name, args, keys=[\"dataset\", \"lmbda\", \"cluster_option\", \"lmbda_tune\", \"L\"]):\n",
    "    \"\"\"\n",
    "    Return entries from the csv file that have the same settings as the passed args, using compare_entry().\n",
    "\n",
    "    csv_name:\n",
    "        filename of the csv, eg \"results.csv\"\n",
    "    args:\n",
    "        arguments that will be passed to main()\n",
    "    \"\"\"\n",
    "    entries = []\n",
    "    csv_path = os.path.join(args.output_path, csv_name)\n",
    "    with open(csv_path, \"r\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            if compare_entry(args, row, keys):\n",
    "                entries.append(row)\n",
    "    return entries\n",
    "\n",
    "def filter_outliers(entries, keys):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Execute one run\"\"\"\n",
    "seed = 1\n",
    "dataset = \"Synthetic-unequal\"\n",
    "cluster_option = \"kmedian\"\n",
    "lmbda = lambdas_by_setting[dataset][cluster_option] # Use standard lambda\n",
    "# lmbda = 1\n",
    "lmbda_tune = False\n",
    "L = 2.0 # Lipschitz constant\n",
    "run_main(get_args(seed, dataset, cluster_option, lmbda, lmbda_tune, L))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Run all settings with different seeds\"\"\"\n",
    "for dataset in lambdas_by_setting:\n",
    "    for cluster_option in lambdas_by_setting[dataset]:\n",
    "        lmbda = lambdas_by_setting[dataset][cluster_option]\n",
    "\n",
    "        args = get_args(dataset=dataset, cluster_option=cluster_option, lmbda=lmbda)\n",
    "        existing_entries = find_same_options(CSV_NAME, args)\n",
    "\n",
    "        if len(existing_entries) >= n_runs[dataset]:\n",
    "            print(\"enough results for these settings\")\n",
    "            continue\n",
    "        n_todo = n_runs[dataset] - len(existing_entries)\n",
    "\n",
    "        seeds = [int(entry[\"seed\"]) for entry in existing_entries]\n",
    "        seeds.append(0) # Make sure seeds is not empty\n",
    "        next_seed = max(seeds) + 1\n",
    "\n",
    "        for new_seed in range(next_seed, next_seed + n_todo):\n",
    "            args.seed = new_seed\n",
    "            print()\n",
    "            run_main(args, CSV_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Fetch results from the csv\"\"\"\n",
    "for dataset in lambdas_by_setting:\n",
    "    print(f\"\\n\\n{dataset}\")\n",
    "    for cluster_option in lambdas_by_setting[dataset]:\n",
    "        print(\"\\n  \"+cluster_option.upper())\n",
    "        lmbda = lambdas_by_setting[dataset][cluster_option]\n",
    "\n",
    "        args = get_args(dataset=dataset, cluster_option=cluster_option, lmbda=lmbda)\n",
    "        existing_entries = find_same_options(CSV_NAME, args)\n",
    "        \n",
    "        if len(existing_entries) < 1:\n",
    "            print(\"no data yet\")\n",
    "            continue\n",
    "\n",
    "        entry = existing_entries[0]\n",
    "        print(f\"  lmbda={lmbda}, Lipschitz={entry['L']}\")\n",
    "        \n",
    "        keys = [\"Objective\", \"fairness error\", \"balance\", \"time\"]\n",
    "        for key in keys:\n",
    "            data = [float(entry[key]) for entry in existing_entries]\n",
    "            mean = np.mean(data)\n",
    "            std = np.std(data)\n",
    "\n",
    "            print(f\"    {key}{' '*(20-len(key))} M = {mean:.2f}     SD = {std:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_Lipschitz_test(args, csv_name=CSV_NAME_LIPSCHITZ):\n",
    "    \"\"\"\n",
    "    Run bound update and append results with settings to csv file.\n",
    "    File is automatically made if it doesn't exist yet.\n",
    "    Logs of energy values by iteration are saved to .txt files as json, the path to which is saved in a csv field.\n",
    "\n",
    "    csv_name:\n",
    "        just the filename of the csv, eg \"results_Lipschitz.csv\"\n",
    "    \"\"\"\n",
    "    args.plot_bound_update = True\n",
    "    bound_energy_list, elapsed = main(args, logging=False, seedable=True)\n",
    "\n",
    "    list_dir = os.path.join(args.dataset, \"Lipschitz_energy_list_logs\")\n",
    "    os.makedirs(os.path.join(args.output_path, list_dir), exist_ok=True)\n",
    "    list_filepath = os.path.join(list_dir, f\"{args.cluster_option}__L={args.L}__lmb={args.lmbda}__seed={args.seed}.txt\")\n",
    "    list_filepath_full = os.path.join(args.output_path, list_filepath)\n",
    "    with open(list_filepath_full, \"w\") as f:\n",
    "        f.write(json.dumps(bound_energy_list))\n",
    "\n",
    "    save_dict = {\n",
    "        \"dataset\": args.dataset,\n",
    "        \"lmbda\": args.lmbda,\n",
    "        \"cluster_option\": args.cluster_option,\n",
    "        \"L\": args.L,                            # Lipschitz constant\n",
    "        \"convergence_iter\": len(bound_energy_list),\n",
    "        \"optimum\": min(bound_energy_list),\n",
    "        \"time\": elapsed,                        # Time taken to finish this run\n",
    "        \"seed\": args.seed,\n",
    "        \"energy_list_file\": list_filepath,        \n",
    "    }\n",
    "\n",
    "    csv_path = os.path.join(args.output_path, csv_name)\n",
    "    fieldnames = save_dict.keys()\n",
    "    make_csv(args.output_path, csv_path, fieldnames)\n",
    "    with open(csv_path, \"a\", newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames)\n",
    "        writer.writerow(save_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_Lipschitz_test(get_args(Lipschitz=0.0001, plot_bound_update=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Run Lipschitz test for multiple settings\"\"\"\n",
    "\n",
    "for dataset in [\"Synthetic\", \"Synthetic-unequal\", \"Adult\", \"Bank\"]:\n",
    "    for cluster_option in [\"kmedian\", \"kmeans\", \"ncut\"]:\n",
    "        for L in Lipschitz_constants:\n",
    "\n",
    "            args = get_args(dataset=dataset, cluster_option=cluster_option, lmbda=1, Lipschitz=L, plot_bound_update=True)\n",
    "            existing_entries = find_same_options(CSV_NAME_LIPSCHITZ, args, keys=[\"dataset\", \"cluster_option\", \"L\"])\n",
    "\n",
    "            if len(existing_entries) >= n_runs_Lipschitz:\n",
    "                print(\"enough results for these settings\")\n",
    "                continue\n",
    "            n_todo = n_runs_Lipschitz - len(existing_entries)\n",
    "\n",
    "            seeds = [int(entry[\"seed\"]) for entry in existing_entries]\n",
    "            seeds.append(0) # Make sure seeds is not empty\n",
    "            next_seed = max(seeds) + 1\n",
    "\n",
    "            for new_seed in range(next_seed, next_seed + n_todo):\n",
    "                args.seed = new_seed\n",
    "                print()\n",
    "                run_Lipschitz_test(args, CSV_NAME_LIPSCHITZ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def plot_Lipschitz_convergence(save_path, energy_list_by_L, yscale_log=False):\n",
    "    \"\"\"Plot clustering objective by iteration for different Lipschitz constants\"\"\"\n",
    "    for L, lis in energy_list_by_L.items():\n",
    "        plt.plot(range(len(lis[\"mean\"])), lis[\"mean\"], label=f\"L = {L}\")\n",
    "        # plt.fill_between(range(len(lis[\"std\"])), lis[\"mean\"]+lis[\"std\"], lis[\"mean\"]-lis[\"std\"], alpha=.5)\n",
    "    plt.legend()\n",
    "    plt.ylabel(\"fair objective\")\n",
    "    plt.xlabel(\"iterations\")\n",
    "    suffix = \"\"\n",
    "    if yscale_log:\n",
    "        plt.ylim(min(energy_list_by_L[2.0][\"mean\"]), np.max([max(lis[\"mean\"][1:]) for lis in energy_list_by_L.values()]))\n",
    "        plt.yscale('log')\n",
    "        suffix = \"_y-log\"\n",
    "    else:\n",
    "        plt.ylim(min(energy_list_by_L[2.0][\"mean\"]), max(energy_list_by_L[0.01][\"mean\"][1:]))\n",
    "    plt.xscale('log')\n",
    "    plt.savefig(save_path.format(suffix=suffix))\n",
    "    plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Fetch Lipschitz results from the csv and make plots\"\"\"\n",
    "for dataset in lambdas_by_setting:\n",
    "    print(\"\\n\\n\"+dataset)\n",
    "    for cluster_option in lambdas_by_setting[dataset]:\n",
    "        print(\"\\n  \"+cluster_option.upper())\n",
    "        energy_list_by_L = {}\n",
    "        complete = True\n",
    "        for L in Lipschitz_constants:\n",
    "            args = get_args(dataset=dataset, cluster_option=cluster_option, Lipschitz=L)\n",
    "            existing_entries = find_same_options(CSV_NAME_LIPSCHITZ, args, keys=[\"dataset\", \"cluster_option\", \"L\"])\n",
    "            \n",
    "            if len(existing_entries) < 1:\n",
    "                print(f\"no data yet on {dataset} with {cluster_option} at Lipshitz={L}\")\n",
    "                complete = False\n",
    "                continue\n",
    "\n",
    "            print(f\"\\n    Lipschitz = {L}\")\n",
    "            keys = [\"convergence_iter\", \"optimum\", \"time\"]\n",
    "            for key in keys:\n",
    "                data = [float(entry[key]) for entry in existing_entries]\n",
    "                mean = np.mean(data)\n",
    "                std = np.std(data)\n",
    "\n",
    "                print(f\"    {key}{' '*(20-len(key))} M = {mean:.2f}     SD = {std:.2f}\")\n",
    "\n",
    "            energy_lists_by_run = []\n",
    "            for entry in existing_entries:\n",
    "                with open(os.path.join(args.output_path, entry[\"energy_list_file\"]), \"r\") as f:\n",
    "                    energy_lists_by_run.append(json.loads(f.read()))\n",
    "\n",
    "            max_len = max([int(entry[\"convergence_iter\"]) for entry in existing_entries])\n",
    "            energy_array_by_run = np.zeros((len(existing_entries), max_len))\n",
    "            for i, energy_list in enumerate(energy_lists_by_run):\n",
    "                last_value = energy_list[-1]\n",
    "                for iter in range(len(energy_list), max_len):\n",
    "                    energy_list.append(last_value)\n",
    "                energy_array_by_run[i] = energy_list\n",
    "                \n",
    "            energy_list_by_L[L] = {\n",
    "                # \"mean\": np.mean(energy_array_by_run, axis=1),\n",
    "                \"mean\": energy_array_by_run[0],\n",
    "                \"std\": np.std(energy_array_by_run, axis=1),\n",
    "            }\n",
    "\n",
    "        if complete:\n",
    "            save_dir = os.path.join(args.output_path, dataset)\n",
    "            save_path = os.path.join(save_dir, f\"{cluster_option}_\"+\"Lipschitz_plot{suffix}.png\")\n",
    "            plot_Lipschitz_convergence(save_path, energy_list_by_L)\n",
    "            plot_Lipschitz_convergence(save_path, energy_list_by_L, yscale_log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d5cc03dcd2e2df7f8793d069ea863b6d1996a0b7026a80ee0313ee24cbfa610e"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('fact_vfc': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
