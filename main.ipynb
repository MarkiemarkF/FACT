{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from test_fair_clustering import main\n",
    "import argparse\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "%matplotlib inline\n",
    "\n",
    "from alter_data import convert_data_columns\n",
    "from alter_data import remove_first_column\n",
    "\n",
    "from notebook_utils.filtering import find_same_options\n",
    "from notebook_utils.saving import make_csv, make_save_dict, make_save_dict_Lipschitz, convert_args_for_kernel_save\n",
    "from notebook_utils.fetching_and_printing import fetch_and_print, fetch_and_print_Lipschitz\n",
    "from data_visualization import plot_fairness_vs_clusterE\n",
    "from src.dataset_load import read_dataset\n",
    "\n",
    "# Imports for Bera et al. baseline\n",
    "import configparser\n",
    "from bera.fair_clustering import fair_clustering\n",
    "from bera.util.configutil import read_list\n",
    "from bera.get_results import convert_bera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_FOLDER       = \"outputs\"\n",
    "DATA_FOLDER         = \"data\"\n",
    "CSV_NAME            = \"results.csv\"\n",
    "CSV_NAME_LIPSCHITZ  = \"results_Lipschitz.csv\"\n",
    "FINAL_RESULTS_NAME  = \"final_results.json\"\n",
    "\n",
    "DEFAULT_RUNS = 30\n",
    "DEFAULT_REPROD_L = 2.0\n",
    "MODES = [\"reprod\", \"lmbda_reprod\", \"Lipz_replic\", \"extra_datasets\", \"kernel_baseline\", \"kernel_poly\", \"kernel_tanh\", \"kernel_radial\"]\n",
    "\n",
    "N_RUNS_LIPSCHITZ = 10\n",
    "LIPSCHITZ_CONSTANTS = [0.00001, 0.0001, 0.001, 0.01, 0.1, 1.0, 2.0]\n",
    "\n",
    "REPRODUCE = False\n",
    "\"\"\"Run everything again if REPRODUCE\"\"\"\n",
    "TEST_RUN = True\n",
    "\"\"\"Run a single test run for all configs if TEST_RUN\"\"\"\n",
    "\n",
    "KERNEL_RUNS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if REPRODUCE:\n",
    "    CSV_NAME = \"new_results.csv\"\n",
    "    CSV_NAME_LIPSCHITZ = \"new_results_Lipschitz.csv\"\n",
    "elif TEST_RUN:\n",
    "    DEFAULT_RUNS = 1\n",
    "    N_RUNS_LIPSCHITZ = 1\n",
    "    # DEFAULT_REPROD_L = 1e-4 # For faster testing\n",
    "    CSV_NAME = \"testrun_results.csv\"\n",
    "    CSV_NAME_LIPSCHITZ = \"testrun_results_Lipschitz.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synthetic kmedian reprod got consistently bad results\n",
    "# Synthetic kmeans reprod gets perfect results only on some seeds\n",
    "# Synthetic-unequal kmeans reprod gets perfect reproduction without seeds 20, 22 and 24\n",
    "# Census II ncut reprod maybe would've worked with lmbda < 100 but not enough time to experiment\n",
    "\n",
    "# Load configs\n",
    "with open(\"notebook_utils/configurations.json\", \"r\") as f:\n",
    "    configs = json.load(f)\n",
    "\n",
    "# Load additional configs just for displaying results\n",
    "with open(\"notebook_utils/configurations_additional.json\", \"r\") as f:\n",
    "    configs_additional = json.load(f)\n",
    "\n",
    "# Preprocess Student dataset\n",
    "base_path = os.path.join('Student', 'student')\n",
    "if os.path.isfile(os.path.join(DATA_FOLDER, base_path+\"-mat.csv\")) and not os.path.isfile(os.path.join(DATA_FOLDER, base_path+\"_mat_Cortez_sexmod.csv\")):\n",
    "    convert_data_columns(base_path+\"-mat.csv\", base_path+\"_mat_Cortez_sexmod.csv\", 1, [('M', 0), ('F', 1)])\n",
    "elif not os.path.isfile(os.path.join(DATA_FOLDER, base_path+\"-mat.csv\")):\n",
    "    print(\"Download the Student dataset as described in README\")\n",
    "\n",
    "# Preprocess Drugnet dataset\n",
    "base_path = os.path.join(os.getcwd(), DATA_FOLDER, 'Drugnet', 'DRUG')\n",
    "if os.path.isfile(base_path+\"ATTR.csv\") and not os.path.isfile(base_path+\"ATTR_mod.csv\"):\n",
    "    remove_first_column(base_path+\"ATTR.csv\", base_path+\"ATTR_mod.csv\")\n",
    "if os.path.isfile(base_path+\"NET.csv\") and not os.path.isfile(base_path+\"NET_mod.csv\"):    \n",
    "    remove_first_column(base_path+\"NET.csv\", base_path+\"NET_mod.csv\", starting_row=1)\n",
    "elif not os.path.isfile(base_path+\"NET.csv\"):\n",
    "    print(\"Download the Drugnet dataset as described in README\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args(seed: int=1, dataset: str=\"Synthetic-unequal\", cluster_option: str=\"ncut\", lmbda: float=None, lmbda_tune: bool=False, Lipschitz: float=None, bound_update_test: bool=False, kernel_type=None) -> argparse.Namespace:\n",
    "    \"\"\"\n",
    "    Return Namespace instance with passed and default arguments for main().\n",
    "\n",
    "    :param bound_update_test: if True, lets main() return a list of energies of a single bound update. Meant for checking convergence.\n",
    "    :param for the rest see test_fair_clustering.py:\n",
    "\n",
    "    :return Namespace instance with passed and default arguments for main()\n",
    "    \"\"\"\n",
    "    args = argparse.Namespace()\n",
    "    \n",
    "    args.plot_option_clusters_vs_lambda = False     # Only available for the synthetic datasets\n",
    "    args.plot_option_fairness_vs_clusterE = True    # Only works when lmbda_tune == True\n",
    "    args.plot_option_balance_vs_clusterE = False    # Only works when lmbda_tune == True\n",
    "    args.plot_option_convergence = False\n",
    "    args.bound_update_test = bound_update_test      # Return a list of the energies of a single bound update\n",
    "    \n",
    "    args.bera = False\n",
    "\n",
    "    if kernel_type:\n",
    "        args.kernel_type = kernel_type\n",
    "    else:\n",
    "        args.kernel_type = \"poly\"\n",
    "    \n",
    "    if args.kernel_type in [\"poly\", \"tanh\"]:\n",
    "        args.kernel_args = \"2_2\"\n",
    "    elif args.kernel_type == \"radial\":\n",
    "        args.kernel_args = \"2_2\"\n",
    "    else:\n",
    "        print(f\"kernel_type: {args.kernel_type} not valid\")\n",
    "\n",
    "    args.seed = seed\n",
    "    args.dataset = dataset\n",
    "    args.cluster_option = cluster_option\n",
    "    args.lmbda_tune = lmbda_tune\n",
    "\n",
    "    if lmbda:\n",
    "        args.lmbda = lmbda\n",
    "    else:\n",
    "        args.lmbda = configs[dataset][cluster_option][\"reprod\"][\"lmbda\"]\n",
    "    \n",
    "    if Lipschitz:\n",
    "        args.L = Lipschitz\n",
    "    elif \"Lipschitz\" in configs[dataset][cluster_option][\"reprod\"]:\n",
    "        args.L = configs[dataset][cluster_option][\"reprod\"][\"Lipschitz\"]\n",
    "    else:\n",
    "        args.L = DEFAULT_REPROD_L\n",
    "\n",
    "    working_dir = os.getcwd()\n",
    "    args.data_dir = os.path.join(working_dir, DATA_FOLDER)\n",
    "    args.output_path = os.path.join(working_dir, OUTPUT_FOLDER)\n",
    "    return args\n",
    "\n",
    "\n",
    "def run_main(args: argparse.Namespace, csv_name: str=CSV_NAME) -> None:\n",
    "    \"\"\"\n",
    "    Run main and append results with config to csv file.\n",
    "    File is automatically made if it doesn't exist yet.\n",
    "\n",
    "    :param args: Namespace instance with arguments for main()\n",
    "    :param csv_name: just the filename of the csv, eg \"results.csv\"\n",
    "    \"\"\"\n",
    "    results = main(args, logging=False, seedable=True)\n",
    "    \n",
    "    save_dict = make_save_dict(convert_args_for_kernel_save(args), results)\n",
    "\n",
    "    # Make csv if necessary and save row\n",
    "    csv_path = os.path.join(args.output_path, csv_name)\n",
    "    fieldnames = save_dict.keys()\n",
    "    make_csv(args.output_path, csv_path, fieldnames)\n",
    "    with open(csv_path, \"a\", newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames)\n",
    "        writer.writerow(save_dict)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Execute one run\"\"\"\n",
    "# run_main(get_args(\n",
    "#     seed = 3, \n",
    "#     dataset = \"CensusII\",\n",
    "#     cluster_option = \"ncut\", \n",
    "#     lmbda = .5,\n",
    "#     # lmbda_tune = True, \n",
    "#     Lipschitz = 1e-5,     # Lipschitz constant\n",
    "# ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Run all configs with different seeds\"\"\"\n",
    "\n",
    "# Make sure the csv is there\n",
    "args = get_args()\n",
    "csv_path = os.path.join(args.output_path, CSV_NAME)\n",
    "fieldnames = make_save_dict(args).keys()\n",
    "make_csv(args.output_path, csv_path, fieldnames)\n",
    "\n",
    "for mode in MODES:\n",
    "    for dataset in configs:\n",
    "        for cluster_option in configs[dataset]:\n",
    "            if mode not in configs[dataset][cluster_option]:\n",
    "                continue\n",
    "\n",
    "            # Get specified parameters for this config, else get the default\n",
    "            lmbda = configs[dataset][cluster_option][mode][\"lmbda\"]\n",
    "            L = DEFAULT_REPROD_L             \n",
    "            if \"Lipschitz\" in configs[dataset][cluster_option][mode]:\n",
    "                L = configs[dataset][cluster_option][mode][\"Lipschitz\"]\n",
    "            n_runs = DEFAULT_RUNS\n",
    "            if \"runs\" in configs[dataset][cluster_option][mode]:\n",
    "                n_runs = min(DEFAULT_RUNS, configs[dataset][cluster_option][mode][\"runs\"])\n",
    "\n",
    "            kernel_type = None\n",
    "            if \"kernel\" in mode:\n",
    "                n_runs = KERNEL_RUNS # For lack of time\n",
    "                if mode.split(\"_\")[1] != \"baseline\":\n",
    "                    kernel_type = mode.split(\"_\")[1]\n",
    "\n",
    "            args = get_args(dataset=dataset, cluster_option=cluster_option, lmbda=lmbda, Lipschitz=L, kernel_type=kernel_type)\n",
    "\n",
    "            # Find existing entries with same config in the csv\n",
    "            check_args = convert_args_for_kernel_save(args)\n",
    "            existing_entries = find_same_options(CSV_NAME, check_args)\n",
    "            if len(existing_entries) >= n_runs:\n",
    "                print(f\"enough {mode} results for {cluster_option} on {dataset}\")\n",
    "                continue\n",
    "\n",
    "            # Run n_todo times with different seeds\n",
    "            n_todo = n_runs - len(existing_entries)\n",
    "\n",
    "            seeds = [int(entry[\"seed\"]) for entry in existing_entries]\n",
    "            seeds.append(0) # Make sure seeds is not empty\n",
    "            next_seed = max(seeds) + 1\n",
    "\n",
    "            for new_seed in range(next_seed, next_seed + n_todo):\n",
    "                args.seed = new_seed\n",
    "                print()\n",
    "                run_main(args, CSV_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Fetch results from the csv\"\"\"\n",
    "\n",
    "fetch_and_print(configs, configs_additional, get_args, MODES, DEFAULT_REPROD_L, CSV_NAME, FINAL_RESULTS_NAME)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Run and plot lmbda tune for different configs\"\"\"\n",
    "\n",
    "for dataset in [\"Adult\", \"Bank\"]:\n",
    "    for cluster_option in [\"kmeans\", \"ncut\"]:\n",
    "        L_list = [2.0]\n",
    "        if cluster_option == \"ncut\":\n",
    "            L_list = [0.001, 2.0]\n",
    "        for L in L_list:\n",
    "            name = f'Fair_{cluster_option}_fairness_vs_clusterEdiscrete_Lip{L}_{dataset}'\n",
    "            savefile = os.path.join(\"outputs\", dataset, name+'.npz')\n",
    "            filename = os.path.join(\"outputs\", dataset, name+'.png')\n",
    "\n",
    "            if not os.path.exists(savefile) or TEST_RUN:\n",
    "                run_main(get_args(1, dataset, cluster_option, lmbda_tune=True, Lipschitz=L))\n",
    "            \n",
    "            print(f\"Lipschitz: {L}\")\n",
    "            plot_fairness_vs_clusterE(cluster_option, savefile, filename, None, None, None, None, None, save=False, save_without_title=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_Lipschitz_test(args, csv_name=CSV_NAME_LIPSCHITZ):\n",
    "    \"\"\"\n",
    "    Run bound update and append results with config to csv file.\n",
    "    File is automatically made if it doesn't exist yet.\n",
    "    Logs of energy values by iteration are saved to json files, the path to which is saved in a csv field.\n",
    "\n",
    "    :param args: Namespace instance with arguments for main()\n",
    "    :param csv_name: just the filename of the csv, eg \"results_Lipschitz.csv\"\n",
    "    \"\"\"\n",
    "    args.bound_update_test = True\n",
    "    bound_energy_list, elapsed = main(args, logging=False, seedable=True)\n",
    "\n",
    "    # Save bound_energy_list separately because it's often too large for csv\n",
    "    list_dir = os.path.join(args.dataset, \"Lipschitz_energy_list_logs\")\n",
    "    os.makedirs(os.path.join(args.output_path, list_dir), exist_ok=True)\n",
    "    list_filepath = os.path.join(list_dir, f\"{args.cluster_option}__L={args.L}__lmb={args.lmbda}__seed={args.seed}.json\")\n",
    "    list_filepath_full = os.path.join(args.output_path, list_filepath)\n",
    "    with open(list_filepath_full, \"w\") as f:\n",
    "        json.dump(bound_energy_list, f, indend=4)\n",
    "\n",
    "    save_dict = make_save_dict_Lipschitz(args, bound_energy_list, elapsed, list_filepath)\n",
    "\n",
    "    # Make csv if necessary and save row\n",
    "    csv_path = os.path.join(args.output_path, csv_name)\n",
    "    fieldnames = save_dict.keys()\n",
    "    make_csv(args.output_path, csv_path, fieldnames)\n",
    "    with open(csv_path, \"a\", newline='') as f:\n",
    "        writer = csv.DictWriter(f, fieldnames)\n",
    "        writer.writerow(save_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Run Lipschitz test for multiple configs\"\"\"\n",
    "\n",
    "# Make sure the csv is there\n",
    "args = get_args()\n",
    "csv_path = os.path.join(args.output_path, CSV_NAME_LIPSCHITZ)\n",
    "fieldnames = make_save_dict_Lipschitz(args).keys()\n",
    "make_csv(args.output_path, csv_path, fieldnames)\n",
    "\n",
    "use_datasets = [\n",
    "    # \"Synthetic\", \n",
    "    # \"Synthetic-unequal\", \n",
    "    \"Adult\", \"Bank\"]\n",
    "use_cluster_options = [\"kmedian\", \"kmeans\", \"ncut\"]\n",
    "for dataset in use_datasets:\n",
    "    for cluster_option in use_cluster_options:\n",
    "        for L in LIPSCHITZ_CONSTANTS:\n",
    "            # Get specified parameters for this config, else get the default\n",
    "            args = get_args(dataset=dataset, cluster_option=cluster_option, lmbda=1, Lipschitz=L, bound_update_test=True)\n",
    "\n",
    "            # Find existing entries with same config in the csv\n",
    "            existing_entries = find_same_options(CSV_NAME_LIPSCHITZ, args, keys=[\"dataset\", \"cluster_option\", \"L\"])\n",
    "            if len(existing_entries) >= N_RUNS_LIPSCHITZ:\n",
    "                print(f\"enough results for bound update test with {cluster_option} on {dataset} with Lipschitz = {L}\")\n",
    "                continue\n",
    "\n",
    "            # Run n_todo times with different seeds\n",
    "            n_todo = N_RUNS_LIPSCHITZ - len(existing_entries)\n",
    "\n",
    "            seeds = [int(entry[\"seed\"]) for entry in existing_entries]\n",
    "            seeds.append(0) # Make sure seeds is not empty\n",
    "            next_seed = max(seeds) + 1\n",
    "\n",
    "            for new_seed in range(next_seed, next_seed + n_todo):\n",
    "                args.seed = new_seed\n",
    "                print()\n",
    "                run_Lipschitz_test(args, CSV_NAME_LIPSCHITZ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Fetch Lipschitz results from the csv and make plots\"\"\"\n",
    "\n",
    "fetch_and_print_Lipschitz(use_datasets, use_cluster_options, get_args, LIPSCHITZ_CONSTANTS, CSV_NAME_LIPSCHITZ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Calculate Bera et al. K-means baseline results\"\"\"\n",
    "\n",
    "dataset = \"Student\"\n",
    "if dataset.lower() not in convert_bera():\n",
    "    config_file = os.path.join(\"bera\", \"config\", \"dataset_configs.ini\")\n",
    "    config = configparser.ConfigParser(converters={'list': read_list})\n",
    "    config.read(config_file)\n",
    "    config_str = dataset.lower()\n",
    "\n",
    "    data, demograph, K = read_dataset(dataset, os.path.join(\"data\", dataset))\n",
    "\n",
    "    print(\"Using config_str = {}\".format(config_str))\n",
    "\n",
    "    # Read variables\n",
    "    output_dir = os.path.join(\"bera\", \"output\")\n",
    "    dataset = config[config_str].get(\"dataset\")\n",
    "    clustering_config_file = config_file\n",
    "    num_clusters = K\n",
    "    deltas = [0.9]\n",
    "    max_points = config[config_str].getint(\"max_points\")\n",
    "    violating = config[\"DEFAULT\"].getboolean(\"violating\")\n",
    "    violation = config[\"DEFAULT\"].getfloat(\"violation\")\n",
    "    violating = False\n",
    "\n",
    "    fair_clustering(config_str, clustering_config_file, output_dir, num_clusters, deltas, max_points, violating, violation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Convert Bera et al. results to Ziko et al. metrics\"\"\"\n",
    "\n",
    "bera_datasets = convert_bera()\n",
    "for dataset in bera_datasets:\n",
    "    if dataset.title() != \"Student\":\n",
    "        continue\n",
    "\n",
    "    args = get_args(dataset=dataset.title(), cluster_option=\"kmeans\", lmbda=\"N/A\", Lipschitz=\"N/A\")\n",
    "    args.bera = True\n",
    "    main(args, logging=False, seedable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d5cc03dcd2e2df7f8793d069ea863b6d1996a0b7026a80ee0313ee24cbfa610e"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('fact_vfc': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
